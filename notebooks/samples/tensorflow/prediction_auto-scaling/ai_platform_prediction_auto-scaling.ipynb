{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46d6c164544f"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2457083dc86a"
      },
      "source": [
        "# Deploying an auto-scaling model with AI Platform Prediction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dd43009ccf"
      },
      "source": [
        "This notebook demonstrates how to deploy a pre-trained model to the AI Platform Prediction service. The notebook will show how to create a new model as well as a new model version. The model version will have auto-scaling settings turned on, so that new nodes will be created and removed as the load changes.\n",
        "\n",
        "We will use a [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder-large/5) model from TensorFlow Hub. This model will create word embeddings from a model that has been trained on a variety of data sources.\n",
        "\n",
        "The notebook itself is adapted from the Universal Sentence Encoder [sample notebook](https://colab.sandbox.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb).\n",
        "\n",
        "The main changes to the sample notebook are:\n",
        "* Creation of AI Platform Prediction model and model version\n",
        "* Update to `embed()` function to use AI Platform Prediction for inference, rather than the local model\n",
        "* Streamlining of some non-essential content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e91bf1aa1b96"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d45211ab6470"
      },
      "outputs": [],
      "source": [
        "# Change these parameters!\n",
        "\n",
        "REGION = 'us-central1' # Update with your region\n",
        "BUCKET = 'gs://<YOUR-BUCKET>' # Update with your bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b35d0e9c978c"
      },
      "outputs": [],
      "source": [
        "# These parameters don't need to be changed\n",
        "\n",
        "MODULE_URL = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\n",
        "MODEL_NAME = 'universal_sentence_encoder'\n",
        "PREDICTIONS_FILE = 'predictions.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3f30142d3a2"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "707d8192aa76"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import datetime\n",
        "import logging\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "551692f9f450"
      },
      "source": [
        "## Download TensorFlow Hub Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46a3f6cd587c"
      },
      "outputs": [],
      "source": [
        "# Reduce logging output\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "\n",
        "# Download model and return path\n",
        "model = hub.resolve(MODULE_URL)\n",
        "\n",
        "print(f\"model file {model} saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32124f591fed"
      },
      "source": [
        "## Deploy AI Platform Prediction model and model version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "910a5cc13ba0"
      },
      "outputs": [],
      "source": [
        "# Create AI Platform Prediction model\n",
        "\n",
        "!gcloud ai-platform models create '{MODEL_NAME}' \\\n",
        "  --region='{REGION}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28c21b33f170"
      },
      "outputs": [],
      "source": [
        "# Create model version string with the current datetime\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "MODEL_VERSION = 'v' + datetime.datetime.strftime(now, '%m%d%Y%H%M%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cd131ff0a11"
      },
      "outputs": [],
      "source": [
        "# Write scaling parameters to config.yaml\n",
        "\n",
        "# Note: these parameters can also be directly specified via the gcloud beta command-line \n",
        "#  --metric-targets cpu-usage=80 \\\n",
        "#  --metric-targets gpu-duty-cycle=80 \\\n",
        "#  --min-nodes 2 \\\n",
        "#  --max-nodes 4\n",
        "\n",
        "CONFIG = '''\n",
        "autoScaling:\n",
        "  minNodes: 2\n",
        "  maxNodes: 4  \n",
        "  metrics:\n",
        "    - name: CPU_USAGE\n",
        "      target: 80  \n",
        "    - name: GPU_DUTY_CYCLE\n",
        "      target: 80\n",
        "'''\n",
        "\n",
        "!echo '{CONFIG}' > config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cce2831ecc8"
      },
      "outputs": [],
      "source": [
        "# Create a new model version. This may take several minutes.\n",
        "\n",
        "!gcloud ai-platform versions create {MODEL_VERSION} \\\n",
        "  --model={MODEL_NAME} \\\n",
        "  --region={REGION} \\\n",
        "  --origin={model} \\\n",
        "  --staging-bucket={BUCKET} \\\n",
        "  --runtime-version=2.2 \\\n",
        "  --framework='TENSORFLOW' \\\n",
        "  --python-version=3.7 \\\n",
        "  --machine-type=n1-standard-4 \\\n",
        "  --accelerator count=1,type=nvidia-tesla-t4 \\\n",
        "  --config=config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc46b845bc74"
      },
      "source": [
        "## Use service to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc41909596d1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def embed(input):\n",
        "    # More info on how to format your input strings:\n",
        "    # https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects/predict\n",
        "    prediction_json = {'instances': input}\n",
        "    \n",
        "    # Export predictions to JSON file\n",
        "    with open(PREDICTIONS_FILE, 'w') as outfile:\n",
        "        json.dump(prediction_json, outfile)    \n",
        "        \n",
        "    # Make predictions\n",
        "    preds = !gcloud ai-platform predict --model {MODEL_NAME} --json-request={PREDICTIONS_FILE} --region={REGION}\n",
        "    \n",
        "    # Convert JSON response into Python object\n",
        "    preds.pop(0) # Remove warning\n",
        "    preds = \"\\n\".join(preds) # Concatenate list of strings into one string\n",
        "    preds = json.loads(preds) # Convert JSON string into Python dict\n",
        "    \n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dabbe79179fd"
      },
      "outputs": [],
      "source": [
        "# Helper functions for plotting\n",
        "\n",
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot(messages_):\n",
        "  message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(messages_, message_embeddings_, 90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "541fbe4f4b38"
      },
      "outputs": [],
      "source": [
        "# Plot the textual similarity between various messages\n",
        "\n",
        "messages = [\n",
        "    # Smartphones\n",
        "    \"I like my phone\",\n",
        "    \"My phone is not good.\",\n",
        "    \"Your cellphone looks great.\",\n",
        "\n",
        "    # Weather\n",
        "    \"Will it snow tomorrow?\",\n",
        "    \"Recently a lot of hurricanes have hit the US\",\n",
        "    \"Global warming is real\",\n",
        "\n",
        "    # Food and health\n",
        "    \"An apple a day, keeps the doctors away\",\n",
        "    \"Eating strawberries is healthy\",\n",
        "    \"Is paleo better than keto?\",\n",
        "\n",
        "    # Asking about age\n",
        "    \"How old are you?\",\n",
        "    \"what is your age?\",\n",
        "]\n",
        "\n",
        "run_and_plot(messages) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bfecb830177"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01a42a99070a"
      },
      "outputs": [],
      "source": [
        "## Delete model version resource\n",
        "!gcloud ai-platform versions delete {MODEL_VERSION} --model {MODEL_NAME} --region {REGION} --quiet \n",
        "\n",
        "# Delete model resource\n",
        "!gcloud ai-platform models delete {MODEL_NAME} --region {REGION} --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ai_platform_prediction_auto-scaling.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
