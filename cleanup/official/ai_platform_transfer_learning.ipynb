{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHLV0D7Y5jtU"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/notebooks/templates/ai_platform_notebooks_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/notebooks/templates/ai_platform_notebooks_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Modern image recognition models have millions of parameters. Training them from scratch requires a lot of labeled training data and a lot of computing power (hundreds of GPU-hours or more). Transfer learning is a technique that shortcuts much of this by taking a piece of a model that has already been trained on a related task and reusing it in a new model. In this tutorial, we will reuse the feature extraction capabilities from powerful image classifiers trained on ImageNet and simply train a new classification layer on top.\n",
        "\n",
        "This tutorial uses TensorFlow Hub to share a pre-trained model.\n",
        "This tutorial demostrates:\n",
        "\n",
        "1. How to use TensorFlow Hub with tf.keras\n",
        "2. How to do image classification using TensorFlow Hub\n",
        "3. How to do simple Transfer learning\n",
        "4. Save model in AI platform bucket\n",
        "\n",
        "### Dataset\n",
        "\n",
        "TensorFlow flowers dataset:\n",
        "\n",
        "- URL: http://download.tensorflow.org/example_images/flower_photos.tgz\n",
        "- DatasetBuilder: tfds.image.flowers.TFFlowers\n",
        "- Version: v1.0.0\n",
        "- Size: 218.21 MiB\n",
        "\n",
        "### Objective\n",
        "\n",
        "Train an image classification model using Transfer Learning; once \n",
        "model has been trained, save it in AI platform. Use \n",
        "TF Hub to retrain the top layer of an existing model to recognize the\n",
        "classes in our dataset.\n",
        "\n",
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
        "\n",
        "* Cloud AI Platform\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Cloud AI Platform\n",
        "pricing](https://cloud.google.com/ml-engine/docs/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## PIP Install Packages and dependencies\n",
        "\n",
        "Install additional dependencies not installed in Notebook environment\n",
        "(e.g. XGBoost, adanet, tf-hub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyy5Lbnzg5fi"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade pip\n",
        "! pip install -U tensorflow_hub --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9luQpONrzPb6"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your GCP project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the AI Platform APIs and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n",
        "\n",
        "4. [Google Cloud SDK](https://cloud.google.com/sdk) is already installed in AI Platform Notebooks.\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your GCP account\n",
        "\n",
        "**If you are using AI Platform Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the GCP Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. From the **Service account** drop-down list, select **New service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name.\n",
        "\n",
        "4. From the **Role** drop-down list, select\n",
        "   **Machine Learning Engine > AI Platform Admin** and\n",
        "   **Storage > Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4561a31436a4"
      },
      "source": [
        "If you are running this notebook in Colab, run the following cell to authenticate your Google Cloud Platform user account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "300f0fefc817"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "    %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a training job using the Cloud SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. AI Platform runs\n",
        "the code from this package. In this tutorial, AI Platform also saves the\n",
        "trained model that results from your job in the same bucket. You can then\n",
        "create an AI Platform model version based on this output in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets. \n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Cloud\n",
        "AI Platform services are\n",
        "available](https://cloud.google.com/ml-engine/docs/tensorflow/regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with AI Platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "# TODO (Set your bucket name)\n",
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "177d41805009"
      },
      "source": [
        "## 1. Obtaining a training dataset\n",
        "\n",
        "For this example, you will be using the Tensorflow flowers dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9692ad42891"
      },
      "outputs": [],
      "source": [
        "data_root = tf.keras.utils.get_file(\n",
        "    \"flower_photos\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n",
        "    untar=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae1a30483471"
      },
      "outputs": [],
      "source": [
        "# Verify folder structure:\n",
        "! ls -al {data_root}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "922e57893887"
      },
      "source": [
        "The simplest way to load this data into our model is using [`tf.keras.preprocessing.image.ImageDataGenerator`](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html),\n",
        "\n",
        "All of TensorFlow Hub's image modules expect float inputs in the [0, 1] range. Use the ImageDataGenerator's rescale parameter to achieve this. The image size will be handled later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bb2705a0061"
      },
      "outputs": [],
      "source": [
        "# Image information\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "CHANNELS = 3\n",
        "IMAGE_SHAPE = (HEIGHT, WIDTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "169a5adb50d8"
      },
      "outputs": [],
      "source": [
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1 / 255)\n",
        "image_data = image_generator.flow_from_directory(\n",
        "    str(data_root), target_size=IMAGE_SHAPE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e2c5a5f11ac"
      },
      "source": [
        "The resulting object is an iterator that returns:\n",
        " - `image_batch, label_batch` pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f19a7dd7596"
      },
      "outputs": [],
      "source": [
        "for image_batch, label_batch in image_data:\n",
        "    print(\"Image batch shape: \", image_batch.shape)\n",
        "    print(\"Label batch shape: \", label_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7932a35e41"
      },
      "source": [
        "## 2. Download the headless model\n",
        "\n",
        "TensorFlow Hub also distributes models without the top classification layer. These can be used to easily do transfer learning.\n",
        "\n",
        "Any [TensorFlow 2.x image feature vector URL](https://tfhub.dev/s?module-type=image-feature-vector&tf-version=tf2) from tfhub.dev will work here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70273ad3a3a6"
      },
      "outputs": [],
      "source": [
        "# TODO: replace the URL with any TF 2.x image feature vector from tfhub.dev\n",
        "feature_extractor_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fef518c32a"
      },
      "source": [
        "Create the layer, and check the expected image size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1780d42038ce"
      },
      "outputs": [],
      "source": [
        "feature_extractor_layer = hub.KerasLayer(\n",
        "    feature_extractor_url, input_shape=(HEIGHT, WIDTH, CHANNELS)\n",
        ")\n",
        "feature_batch = feature_extractor_layer(image_batch)\n",
        "print(feature_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4078d7d387fd"
      },
      "source": [
        "The feature extractor returns a 1280-element vector for each image.\n",
        "\n",
        "Freeze the variables in the feature extractor layer, so that the training only modifies the new classifier layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9154043493e2"
      },
      "outputs": [],
      "source": [
        "feature_extractor_layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2b9f71808ff"
      },
      "source": [
        "## 3. Attach a classification head\n",
        "\n",
        "Now wrap the hub layer in a tf.keras.Sequential model, and add a new classification layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d6ff450043e"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        feature_extractor_layer,\n",
        "        layers.Dense(image_data.num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b68a7cb76d2d"
      },
      "outputs": [],
      "source": [
        "predictions = model(image_batch)\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e1f72dad74a"
      },
      "source": [
        "## 4. Train the model\n",
        "\n",
        "Use compile to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71d73c36f7df"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"acc\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6f0a58b2c93"
      },
      "source": [
        "Now use the .fit method to train the model.\n",
        "\n",
        "To keep this example short, train just 5 epochs. To visualize the training progress, use a custom callback to log the loss and accuracy of each batch individually, instead of the epoch average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94b05cfc9d97"
      },
      "outputs": [],
      "source": [
        "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.batch_losses = []\n",
        "        self.batch_acc = []\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        self.batch_losses.append(logs[\"loss\"])\n",
        "        self.batch_acc.append(logs[\"acc\"])\n",
        "        self.model.reset_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1848bd073a4e"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = np.ceil(image_data.samples / image_data.batch_size)\n",
        "batch_stats_callback = CollectBatchStats()\n",
        "\n",
        "history = model.fit(\n",
        "    image_data,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[batch_stats_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bddfdfe4fe"
      },
      "source": [
        "Now after, even just a few training iterations, we can already see that the model is making progress on the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1048ea1f22c"
      },
      "outputs": [],
      "source": [
        "# Plotting loss\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0, 2])\n",
        "plt.plot(batch_stats_callback.batch_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42cd9f2d3534"
      },
      "outputs": [],
      "source": [
        "# Plotting accuracy\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0, 1])\n",
        "plt.plot(batch_stats_callback.batch_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5bb7c188e80"
      },
      "source": [
        "## 5. Check the predictions\n",
        "\n",
        "To redo the plot from before, first get the ordered list of class names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9edbe8a966de"
      },
      "outputs": [],
      "source": [
        "class_names = sorted(image_data.class_indices.items(), key=lambda pair: pair[1])\n",
        "class_names = np.array([key.title() for key, value in class_names])\n",
        "class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c805da23069"
      },
      "source": [
        "Run the image batch through the model and convert the indices to class names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fafd023a386"
      },
      "outputs": [],
      "source": [
        "predicted_batch = model.predict(image_batch)\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
        "predicted_label_batch = class_names[predicted_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3933b82251a"
      },
      "source": [
        "Plot the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69375e44fe14"
      },
      "outputs": [],
      "source": [
        "label_id = np.argmax(label_batch, axis=-1)\n",
        "plt.figure(figsize=(10, 9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for n in range(30):\n",
        "    plt.subplot(6, 5, n + 1)\n",
        "    plt.imshow(image_batch[n])\n",
        "    color = \"green\" if predicted_id[n] == label_id[n] else \"red\"\n",
        "    plt.title(predicted_label_batch[n].title(), color=color)\n",
        "    plt.axis(\"off\")\n",
        "_ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcf8b88a1a83"
      },
      "source": [
        "## 6. Save your model\n",
        "\n",
        "Now that you've trained the model, save it in \"tf\" format (this is the default format for TF 2.x). Export the model to user defined bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5554140d2b7a"
      },
      "outputs": [],
      "source": [
        "# TODO: label version as needed\n",
        "VERSION = 1565292863\n",
        "\n",
        "tf_export_path = \"gs://{}/saved_models/{}\".format(BUCKET_NAME, VERSION)\n",
        "tf.keras.models.save_model(\n",
        "    model, tf_export_path, include_optimizer=True, save_format=\"tf\"\n",
        ")\n",
        "\n",
        "# Print directory where model is located\n",
        "tf_export_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db59f3f5e2c1"
      },
      "source": [
        "Verify that the model has been saved successfully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d11aa082bdc"
      },
      "outputs": [],
      "source": [
        "# TODO (Set your bucket and version)\n",
        "! gsutil ls -l $tf_export_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all GCP resources used in this project, you can [delete the GCP\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "{Include commands to delete individual resources below}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete model from bucket\n",
        "# TODO (Set your bucket and version)\n",
        "! gsutil rm -r $tf_export_path"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ai_platform_transfer_learning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
